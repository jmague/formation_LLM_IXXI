{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[comment]: <> \"LTeX: language=fr\"\n",
    "# <center> Formation IXXI - Grands modèles de langage </center>\n",
    "## <center> Jean-Philippe Magué </center>\n",
    "\n",
    "<div align=\"center\">\n",
    "\t<img src=\"img/logo_ixxi.png\">\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "Cette formation s'appuie sur deux notebooks. Ce premier propose une vision de \"haut niveau\" du fonctionnement du modèle [GPT2](https://huggingface.co/openai-community/gpt2), c'est-à-dire en regardant les entrées et les sorties, mais sans rentrer dans le fonctionnement interne. Le second entre dans les détails internes du modèle. \n",
    "\n",
    "## <center> LLM : Vision de haut niveau </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces notebooks utilisent la bibliothèque [transformers](https://huggingface.co/docs/transformers/en/index) de [HuggingFace](https://huggingface.co/). Cette bibliothèque permet de manipuler une grande variété de modèles, notamment ceux disponibles sur [HuggingFace](https://huggingface.co/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  import subprocess\n",
    "  import sys\n",
    "\n",
    "  def install(package):\n",
    "      subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "  install('accelerate')\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x178f0f750>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False) #no training today!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On charge un modèle pré-entrainé à partir de son nom\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_name) \n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(model_name,device_map=\"auto\",pad_token_id=tokenizer.eos_token_id)# device_map=\"auto\" permet d'utiliser le GPU si il est disponible\n",
    "device = gpt2.device \n",
    "device #indique si le GPU est utilisé (cuda ou mps) ou non (cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Premières générations de texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[comment]: <> \"LTeX: language=fr\"\n",
    "## Tokenisation\n",
    "Les modèles de langue ne traitent pas directement des chaînes de caractères. Celles-ci sont **tokenisées** et ce sont les tokens qui consituent les unités traitées par ces modèles. La liste de ces tokens est finie (il y en a 50257 pour GPT2) et ils sont définis avant même l'entraîment du modèle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "il y a 50257 tokens dans le vocabulaire\n"
     ]
    }
   ],
   "source": [
    "print(f'il y a {gpt2.config.vocab_size} tokens dans le vocabulaire')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Artificial intelligence'\n",
    "prompt = 'ArtificCial intElliGence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 8001,   811,    34,   498,   493,    36, 15516,    38,   594]],\n",
       "       device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]], device='mps:0')}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = tokenizer(prompt, return_tensors=\"pt\").to(device) \n",
    "# return_tensors=\"pt\" indique que l'on veut des tenseurs pytorch\n",
    "# to(device) permet de mettre les données sur le GPU si il est disponible\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|token|  -> id\n",
      "\n",
      "|Art|    -> 8001\n",
      "|ific|   -> 811\n",
      "|C|      -> 34\n",
      "|ial|    -> 498\n",
      "| int|   -> 493\n",
      "|E|      -> 36\n",
      "|lli|    -> 15516\n",
      "|G|      -> 38\n",
      "|ence|   -> 594\n"
     ]
    }
   ],
   "source": [
    "print(f'{\"|token|\":8} -> id\\n')\n",
    "for token in input['input_ids'][0]:\n",
    "    print(f'{\"|\"+tokenizer.decode(token)+\"|\":8} -> {token}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[comment]: <> \"LTeX: language=fr\"\n",
    "#### Un mot sur les batchs\n",
    "Ces modèles sont prévus pour traiter plusieurs entrées (plusieurs prompts) en même temps. On appelle l'ensemble de ces entrées un *batch*. Dans ce notebook et le suivant, nous ne traiterons toujours qu'une seule entrée à la fois (nos batchs ont une taille de 1).\n",
    "\n",
    "Dans la cellule ci-dessous, pour accéder aux tokens de l'entrée, on utilise :\n",
    "\n",
    "```python \n",
    "input['input_ids'][0]\n",
    "```\n",
    "\n",
    "L'indice `0`indique que l'on s'intéresse à la première (et unique) des entrées du batch. Cet indice apparaîtra à de nombreuses reprises dans ces notebooks.\n",
    "\n",
    "Par ailleurs, il faut que les entrées d'un batch aient toutes le même nombre de tokens. Quand ce n'est pas le cas, le modèle complète les plus courtes avec un token spécial, nommé *padding token*. D'où le paramètre `pad_token_id=tokenizer.eos_token_id`lors de la création du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = gpt2.generate(input_ids=input['input_ids'], attention_mask=input['attention_mask'], max_new_tokens=1)#le attention_mask permet de dire au de ne pas prêter attention aux tokens de padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8001, 9542, 4430,  318]], device='mps:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[comment]: <> \"LTeX: language=fr\"\n",
    "Le modèle renvoie également une liste (d'identifiants) de tokens : ceux donnés en entrée plus celui généré. Le tokenizer permet de les décoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|token|  -> id\n",
      "\n",
      "|Art|    -> 8001\n",
      "|ificial| -> 9542\n",
      "| intelligence| -> 4430\n",
      "| is|    -> 318\n"
     ]
    }
   ],
   "source": [
    "print(f'{\"|token|\":8} -> id\\n')\n",
    "for token in output[0]:\n",
    "    print(f'{\"|\"+tokenizer.decode(token)+\"|\":8} -> {token}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si l'on souhaite générer plus de 1 token, il suffit de répéter l'opération:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial intelligence is a'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2 = gpt2.generate(input_ids=output, attention_mask=torch.ones(output.shape).to(device), max_new_tokens=1)\n",
    "tokenizer.decode(output2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial intelligence is a new'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output3 = gpt2.generate(input_ids=output2, attention_mask=torch.ones(output2.shape).to(device), max_new_tokens=1)\n",
    "tokenizer.decode(output3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial intelligence is a new field'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output4 = gpt2.generate(input_ids=output3, attention_mask=torch.ones(output3.shape).to(device), max_new_tokens=1)\n",
    "tokenizer.decode(output4[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou alors, plus simplement : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial intelligence is a new field of research'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output10 = gpt2.generate(input_ids=input['input_ids'], attention_mask=input['attention_mask'], max_new_tokens=6)\n",
    "tokenizer.decode(output10[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[comment]: <> \"LTeX: language=fr\"\n",
    "# Stratégies de génération de texte\n",
    "Ce que calcule le modèle est en fait un score associé à chacun des 50257 tokens à partir duquel il est possible de dériver une distribution de probabilité sur les tokens. Le choix du token qui sera généré est effectué à partir de ces scores/probabilités. On peut demander au modèle de renvoyer ces scores avec les paramètres `return_dict_in_generate=True`, `output_scores=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = gpt2.generate(input_ids=input['input_ids'], attention_mask=input['attention_mask'], max_new_tokens=1,return_dict_in_generate=True, output_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ -99.0574,  -97.3374, -104.6698,  ..., -105.8733, -106.3209,\n",
       "           -99.8503]], device='mps:0'),)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['scores']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[comment]: <> \"LTeX: language=fr\"\n",
    "Ces scores (*logits*) sont transformés en probabilités via une fonction [softmax](https://stackoverflow.com/a/47763299/1731620) :\n",
    "\n",
    "$\\sigma(x_i)=\\frac{e^{x_i}}{\\sum{e^{x_j}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = torch.nn.functional.softmax(output['scores'][0],dim=1)\n",
    "prob=prob[0].cpu()\n",
    "prob_of_each_token = list(zip([tokenizer.decode(i) for i in range(50257)], prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[comment]: <> \"LTeX: language=fr\"\n",
    "On peut regarder quels sont les 10 tokens avec la plus haute probabilité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|token|   -> probability\n",
      "\n",
      "| is|     -> 0.1954\n",
      "| (|      -> 0.0700\n",
      "|,|       -> 0.0597\n",
      "| has|    -> 0.0580\n",
      "| and|    -> 0.0462\n",
      "| will|   -> 0.0447\n",
      "| can|    -> 0.0371\n",
      "| could|  -> 0.0173\n",
      "|.|       -> 0.0166\n",
      "| may|    -> 0.0135\n"
     ]
    }
   ],
   "source": [
    "print(f'{\"|token|\":9} -> probability\\n')\n",
    "for token, p in sorted(prob_of_each_token, key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f'{\"|\"+token+\"|\":9} -> {p:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[comment]: <> \"LTeX: language=fr\"\n",
    "[Différentes stratégies sont possibles](https://huggingface.co/blog/how-to-generate) pour générer le prochain token. La stratégie par défaut employée ici, **greedy search**, consiste tout simplement à choisir à chaque étape le token ayant la plus grande probabilité :\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jmague/formation_LLM_IXXI/master/img/images/images.001.png\" width=\"600\">\n",
    "\n",
    "\n",
    "Elle atteint assez vite ses limites : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial intelligence is a new field of research that has been in the works for a while now. It is a field that has been in the works for a while now. It is a field that has been in the works for a while now. It is a'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = gpt2.generate(input_ids=input['input_ids'], attention_mask=input['attention_mask'], max_new_tokens=50)\n",
    "tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[comment]: <> \"LTeX: language=fr\"\n",
    "### Un peu de stochasticité\n",
    "On peut être moins détermistes et profiter de la distribution de probabilité pour introduire de l'aléatoire :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial intelligence (AI) and machine learning techniques have been found in some papers to be useful in predicting people\\'s views of robots and robots and robots. In the early paper on AI researchers pointed out the potential for these techniques to improve on the old \"predict'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = gpt2.generate(input_ids=input['input_ids'], attention_mask=input['attention_mask'], max_new_tokens=50, do_sample=True)\n",
    "tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[comment]: <> \"LTeX: language=fr\"\n",
    "### Beam search\n",
    "Cette stratégie n'est pas la plus efficace. La séquence de n tokens la plus probable peut ne pas commencer par le token le plus probable :\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jmague/formation_LLM_IXXI/master/img/images/images.002.png\" width=\"600\">\n",
    "\n",
    "Le principe du **Beam search** est de générer en parallèle plusieurs séquences de tokens, de manière à produire, *in fine*, la plus probable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial intelligence (AI) is becoming more and more popular in the digital age.\\n\\nAccording to a new report by the American Society of Artificial Intelligence (ASI), AI is becoming more and more popular in the digital age. According to a new report by the American Society of Artificial Intelligence (ASI), AI is becoming more and more popular in the digital'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#num_beams : nombre de sequences paralleles maintenues\n",
    "output = gpt2.generate(input_ids=input['input_ids'], attention_mask=input['attention_mask'], max_new_tokens=70, num_beams = 5, do_sample=True)\n",
    "tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of Australia?\n",
      "\n",
      "The capital of Australia is the state capital of Australia. The capital of Australia is the state capital of Australia. The capital of Australia is the state capital of Australia. The capital of Australia is the state capital of Australia. The capital of Australia\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the capital of Australia?\"\n",
    "input = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "output = gpt2.generate(input_ids=input['input_ids'], attention_mask=input['attention_mask'], max_new_tokens=50, num_beams = 5, do_sample=True)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "formation_LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
